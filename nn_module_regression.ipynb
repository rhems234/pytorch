{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib as mlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "x_data=torch.FloatTensor([[1],[2],[3],[4],[5]])\n",
    "t_data=torch.FloatTensor([[3],[5],[7],[9],[11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4414], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "model=nn.Linear(1,1)    # input_dim=1, output_dim=1\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0 Cost :  39.16685104370117\n",
      "Epoch :  400 Cost :  0.01109258271753788\n",
      "Epoch :  800 Cost :  0.0007385412463918328\n",
      "Epoch :  1200 Cost :  4.91717473778408e-05\n",
      "Epoch :  1600 Cost :  3.276181814726442e-06\n",
      "Epoch :  2000 Cost :  2.1860606125301274e-07\n",
      "Epoch :  2400 Cost :  1.4702209227834828e-08\n",
      "Epoch :  2800 Cost :  9.986707016906848e-10\n",
      "Epoch :  3200 Cost :  1.0809344391793374e-10\n",
      "Epoch :  3600 Cost :  5.426272625674855e-11\n",
      "Epoch :  4000 Cost :  5.426272625674855e-11\n",
      "Epoch :  4400 Cost :  5.426272625674855e-11\n",
      "Epoch :  4800 Cost :  5.426272625674855e-11\n",
      "Epoch :  5200 Cost :  5.426272625674855e-11\n",
      "Epoch :  5600 Cost :  5.426272625674855e-11\n",
      "Epoch :  6000 Cost :  5.426272625674855e-11\n",
      "Epoch :  6400 Cost :  5.426272625674855e-11\n",
      "Epoch :  6800 Cost :  5.426272625674855e-11\n",
      "Epoch :  7200 Cost :  5.426272625674855e-11\n",
      "Epoch :  7600 Cost :  5.426272625674855e-11\n",
      "Epoch :  8000 Cost :  5.426272625674855e-11\n",
      "Epoch :  8400 Cost :  5.426272625674855e-11\n",
      "Epoch :  8800 Cost :  5.426272625674855e-11\n",
      "Epoch :  9200 Cost :  5.426272625674855e-11\n",
      "Epoch :  9600 Cost :  5.426272625674855e-11\n",
      "Epoch :  10000 Cost :  5.426272625674855e-11\n",
      "Epoch :  10400 Cost :  5.426272625674855e-11\n",
      "Epoch :  10800 Cost :  5.426272625674855e-11\n",
      "Epoch :  11200 Cost :  5.426272625674855e-11\n",
      "Epoch :  11600 Cost :  5.426272625674855e-11\n",
      "Epoch :  12000 Cost :  5.426272625674855e-11\n",
      "Epoch :  12400 Cost :  5.426272625674855e-11\n",
      "Epoch :  12800 Cost :  5.426272625674855e-11\n",
      "Epoch :  13200 Cost :  5.426272625674855e-11\n",
      "Epoch :  13600 Cost :  5.426272625674855e-11\n",
      "Epoch :  14000 Cost :  5.426272625674855e-11\n",
      "Epoch :  14400 Cost :  5.426272625674855e-11\n",
      "Epoch :  14800 Cost :  5.426272625674855e-11\n",
      "Epoch :  15200 Cost :  5.426272625674855e-11\n",
      "Epoch :  15600 Cost :  5.426272625674855e-11\n",
      "Epoch :  16000 Cost :  5.426272625674855e-11\n",
      "Epoch :  16400 Cost :  5.426272625674855e-11\n",
      "Epoch :  16800 Cost :  5.426272625674855e-11\n",
      "Epoch :  17200 Cost :  5.426272625674855e-11\n",
      "Epoch :  17600 Cost :  5.426272625674855e-11\n",
      "Epoch :  18000 Cost :  5.426272625674855e-11\n",
      "Epoch :  18400 Cost :  5.426272625674855e-11\n",
      "Epoch :  18800 Cost :  5.426272625674855e-11\n",
      "Epoch :  19200 Cost :  5.426272625674855e-11\n",
      "Epoch :  19600 Cost :  5.426272625674855e-11\n"
     ]
    }
   ],
   "source": [
    "nb_epochs=20000\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    prediction=model(x_data)\n",
    "    cost=F.mse_loss(prediction, t_data)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 400 == 0:\n",
    "        print('Epoch : ', epoch, 'Cost : ',cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[21.0000]], grad_fn=<AddmmBackward0>)\n",
      "[Parameter containing:\n",
      "tensor([[2.0000]], requires_grad=True), Parameter containing:\n",
      "tensor([1.0000], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "new_data=torch.FloatTensor([[10.0]])\n",
    "y_pred=model(new_data)\n",
    "print(y_pred)\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train =  torch.FloatTensor([[73,  80,  75], \n",
    "[93,  88, 93], \n",
    "[89,  91, 80], \n",
    "[96,  98, 100], \n",
    "[73,  66, 70]]) \n",
    "y_train =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.1119,  0.2710, -0.5435]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3462], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "model=nn.Linear(3,1)\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.SGD(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0 Cost :  41675.6171875\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[-0.0771,  0.3058, -0.5091]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3467], requires_grad=True)]\n",
      "Epoch :  400 Cost :  5.383914470672607\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.6968, 1.0695, 0.2519]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3556], requires_grad=True)]\n",
      "Epoch :  800 Cost :  5.277735233306885\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7010, 1.0646, 0.2526]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3556], requires_grad=True)]\n",
      "Epoch :  1200 Cost :  5.174044132232666\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7051, 1.0597, 0.2534]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3556], requires_grad=True)]\n",
      "Epoch :  1600 Cost :  5.072737693786621\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7092, 1.0549, 0.2541]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3557], requires_grad=True)]\n",
      "Epoch :  2000 Cost :  4.9737868309021\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7133, 1.0502, 0.2547]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3557], requires_grad=True)]\n",
      "Epoch :  2400 Cost :  4.876993656158447\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7173, 1.0455, 0.2554]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3557], requires_grad=True)]\n",
      "Epoch :  2800 Cost :  4.782361030578613\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7214, 1.0408, 0.2560]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3557], requires_grad=True)]\n",
      "Epoch :  3200 Cost :  4.689902305603027\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7253, 1.0363, 0.2566]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3557], requires_grad=True)]\n",
      "Epoch :  3600 Cost :  4.599501609802246\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7293, 1.0318, 0.2572]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3557], requires_grad=True)]\n",
      "Epoch :  4000 Cost :  4.511154651641846\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7332, 1.0273, 0.2577]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3557], requires_grad=True)]\n",
      "Epoch :  4400 Cost :  4.4248433113098145\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7370, 1.0229, 0.2582]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3557], requires_grad=True)]\n",
      "Epoch :  4800 Cost :  4.3404011726379395\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7409, 1.0186, 0.2587]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3557], requires_grad=True)]\n",
      "Epoch :  5200 Cost :  4.257814884185791\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7447, 1.0143, 0.2592]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3558], requires_grad=True)]\n",
      "Epoch :  5600 Cost :  4.1770853996276855\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7485, 1.0101, 0.2597]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3558], requires_grad=True)]\n",
      "Epoch :  6000 Cost :  4.098150730133057\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7522, 1.0059, 0.2601]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3558], requires_grad=True)]\n",
      "Epoch :  6400 Cost :  4.020993232727051\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7559, 1.0018, 0.2605]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3558], requires_grad=True)]\n",
      "Epoch :  6800 Cost :  3.9455063343048096\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7596, 0.9978, 0.2609]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3558], requires_grad=True)]\n",
      "Epoch :  7200 Cost :  3.8716704845428467\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7632, 0.9938, 0.2612]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3558], requires_grad=True)]\n",
      "Epoch :  7600 Cost :  3.7994353771209717\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7668, 0.9898, 0.2616]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3558], requires_grad=True)]\n",
      "Epoch :  8000 Cost :  3.7288177013397217\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7704, 0.9859, 0.2619]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3558], requires_grad=True)]\n",
      "Epoch :  8400 Cost :  3.6596875190734863\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7740, 0.9820, 0.2622]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3559], requires_grad=True)]\n",
      "Epoch :  8800 Cost :  3.5921177864074707\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7775, 0.9782, 0.2625]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3559], requires_grad=True)]\n",
      "Epoch :  9200 Cost :  3.525967836380005\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7810, 0.9745, 0.2627]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3559], requires_grad=True)]\n",
      "Epoch :  9600 Cost :  3.4612674713134766\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7845, 0.9707, 0.2630]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3559], requires_grad=True)]\n",
      "Epoch :  10000 Cost :  3.397965669631958\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7879, 0.9671, 0.2632]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3559], requires_grad=True)]\n",
      "Epoch :  10400 Cost :  3.3360111713409424\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7913, 0.9635, 0.2634]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3559], requires_grad=True)]\n",
      "Epoch :  10800 Cost :  3.2753796577453613\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7947, 0.9599, 0.2636]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3559], requires_grad=True)]\n",
      "Epoch :  11200 Cost :  3.2161006927490234\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.7981, 0.9564, 0.2638]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3559], requires_grad=True)]\n",
      "Epoch :  11600 Cost :  3.1580586433410645\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8014, 0.9529, 0.2639]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  12000 Cost :  3.1012392044067383\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8047, 0.9494, 0.2641]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  12400 Cost :  3.0455925464630127\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8080, 0.9460, 0.2642]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  12800 Cost :  2.9911892414093018\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8112, 0.9427, 0.2643]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  13200 Cost :  2.937896728515625\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8144, 0.9394, 0.2644]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  13600 Cost :  2.8857407569885254\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8176, 0.9361, 0.2644]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  14000 Cost :  2.834695816040039\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8208, 0.9329, 0.2645]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  14400 Cost :  2.784696340560913\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8239, 0.9297, 0.2645]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  14800 Cost :  2.735752582550049\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8270, 0.9266, 0.2646]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  15200 Cost :  2.6877894401550293\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8301, 0.9235, 0.2646]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  15600 Cost :  2.640873432159424\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8332, 0.9204, 0.2646]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  16000 Cost :  2.5949161052703857\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8362, 0.9174, 0.2646]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  16400 Cost :  2.549868106842041\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8392, 0.9144, 0.2646]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  16800 Cost :  2.505800724029541\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8422, 0.9114, 0.2645]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  17200 Cost :  2.462614059448242\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8452, 0.9085, 0.2645]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  17600 Cost :  2.420322895050049\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8481, 0.9056, 0.2644]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  18000 Cost :  2.37886643409729\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8510, 0.9028, 0.2643]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  18400 Cost :  2.338289737701416\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8539, 0.9000, 0.2642]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  18800 Cost :  2.2985286712646484\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8568, 0.8972, 0.2641]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  19200 Cost :  2.259565830230713\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8596, 0.8945, 0.2640]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "Epoch :  19600 Cost :  2.2214016914367676\n",
      "Prmaters: [Parameter containing:\n",
      "tensor([[0.8624, 0.8918, 0.2639]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "nb_epochs=20000\n",
    "for epoch in range(nb_epochs):\n",
    "    prediction=model(x_train)\n",
    "    cost=F.mse_loss(prediction, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 400 == 0:\n",
    "        print('Epoch : ', epoch, 'Cost : ', cost.item())\n",
    "        print('Prmaters:', list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.8652, 0.8891, 0.2638]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n",
      "tensor([[164.4436]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))\n",
    "new_data=torch.FloatTensor([[80,80,90]])\n",
    "y_pred=model(new_data)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class 사용 single regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "x_data=torch.FloatTensor([[1],[2],[3],[4],[5]])\n",
    "t_data=torch.FloatTensor([[3],[5],[7],[9],[11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(1,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4414], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "model=LinearRegressionModel()\n",
    "print(list(model.parameters()))\n",
    "optimizer=optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0 Cost :  tensor(39.1669, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  400 Cost :  tensor(0.0111, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  800 Cost :  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  1200 Cost :  tensor(4.9172e-05, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  1600 Cost :  tensor(3.2762e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  2000 Cost :  tensor(2.1861e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  2400 Cost :  tensor(1.4702e-08, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  2800 Cost :  tensor(9.9867e-10, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  3200 Cost :  tensor(1.0809e-10, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  3600 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  4000 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  4400 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  4800 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  5200 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  5600 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  6000 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  6400 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  6800 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  7200 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  7600 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  8000 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  8400 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  8800 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  9200 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  9600 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  10000 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  10400 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  10800 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  11200 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  11600 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  12000 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  12400 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  12800 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  13200 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  13600 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  14000 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  14400 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  14800 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  15200 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  15600 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  16000 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  16400 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  16800 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  17200 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  17600 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  18000 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  18400 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  18800 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  19200 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  19600 Cost :  tensor(5.4263e-11, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nb_epochs=20000\n",
    "for epoch in range(nb_epochs):\n",
    "    prediction=model(x_data)\n",
    "    cost=F.mse_loss(prediction, t_data)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 400 == 0 :\n",
    "        print('Epoch : ', epoch, 'Cost : ' ,cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[21.0000]], grad_fn=<AddmmBackward0>)\n",
      "[Parameter containing:\n",
      "tensor([[2.0000]], requires_grad=True), Parameter containing:\n",
      "tensor([1.0000], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "new_data=torch.FloatTensor([[10]])\n",
    "pred_y=model(new_data)\n",
    "print(pred_y)\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train =  torch.FloatTensor([[73,  80,  75], \n",
    "                                [93,  88, 93], \n",
    "                                [89,  91, 80], \n",
    "                                [96,  98, 100], \n",
    "                                [73,  66, 70]]) \n",
    "y_train =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(3,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MultiLinearRegressionModel()\n",
    "optimizer=optim.SGD(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0 Cost :  tensor(41675.6172, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  100 Cost :  tensor(11.4129, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  200 Cost :  tensor(5.4387, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  300 Cost :  tensor(5.4108, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  400 Cost :  tensor(5.3839, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  500 Cost :  tensor(5.3571, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  600 Cost :  tensor(5.3305, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  700 Cost :  tensor(5.3040, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  800 Cost :  tensor(5.2777, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  900 Cost :  tensor(5.2516, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  1000 Cost :  tensor(5.2256, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  1100 Cost :  tensor(5.1998, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  1200 Cost :  tensor(5.1740, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  1300 Cost :  tensor(5.1485, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  1400 Cost :  tensor(5.1231, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  1500 Cost :  tensor(5.0978, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1600 Cost :  tensor(5.0727, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  1700 Cost :  tensor(5.0478, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  1800 Cost :  tensor(5.0230, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  1900 Cost :  tensor(4.9983, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  2000 Cost :  tensor(4.9738, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  2100 Cost :  tensor(4.9493, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  2200 Cost :  tensor(4.9251, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  2300 Cost :  tensor(4.9010, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  2400 Cost :  tensor(4.8770, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  2500 Cost :  tensor(4.8531, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  2600 Cost :  tensor(4.8294, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  2700 Cost :  tensor(4.8058, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  2800 Cost :  tensor(4.7824, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  2900 Cost :  tensor(4.7591, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  3000 Cost :  tensor(4.7359, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  3100 Cost :  tensor(4.7128, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  3200 Cost :  tensor(4.6899, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  3300 Cost :  tensor(4.6671, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  3400 Cost :  tensor(4.6444, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  3500 Cost :  tensor(4.6219, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  3600 Cost :  tensor(4.5995, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  3700 Cost :  tensor(4.5772, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  3800 Cost :  tensor(4.5551, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  3900 Cost :  tensor(4.5331, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  4000 Cost :  tensor(4.5112, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  4100 Cost :  tensor(4.4894, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  4200 Cost :  tensor(4.4678, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  4300 Cost :  tensor(4.4463, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  4400 Cost :  tensor(4.4248, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  4500 Cost :  tensor(4.4036, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  4600 Cost :  tensor(4.3824, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  4700 Cost :  tensor(4.3613, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  4800 Cost :  tensor(4.3404, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  4900 Cost :  tensor(4.3196, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  5000 Cost :  tensor(4.2989, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  5100 Cost :  tensor(4.2783, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  5200 Cost :  tensor(4.2578, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  5300 Cost :  tensor(4.2375, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  5400 Cost :  tensor(4.2172, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  5500 Cost :  tensor(4.1971, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  5600 Cost :  tensor(4.1771, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  5700 Cost :  tensor(4.1572, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  5800 Cost :  tensor(4.1374, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  5900 Cost :  tensor(4.1178, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  6000 Cost :  tensor(4.0982, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  6100 Cost :  tensor(4.0787, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  6200 Cost :  tensor(4.0593, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  6300 Cost :  tensor(4.0401, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  6400 Cost :  tensor(4.0210, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  6500 Cost :  tensor(4.0019, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  6600 Cost :  tensor(3.9830, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  6700 Cost :  tensor(3.9642, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  6800 Cost :  tensor(3.9455, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  6900 Cost :  tensor(3.9269, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  7000 Cost :  tensor(3.9084, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  7100 Cost :  tensor(3.8900, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  7200 Cost :  tensor(3.8717, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  7300 Cost :  tensor(3.8535, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  7400 Cost :  tensor(3.8354, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  7500 Cost :  tensor(3.8173, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  7600 Cost :  tensor(3.7994, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  7700 Cost :  tensor(3.7816, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  7800 Cost :  tensor(3.7639, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  7900 Cost :  tensor(3.7463, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  8000 Cost :  tensor(3.7288, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  8100 Cost :  tensor(3.7114, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  8200 Cost :  tensor(3.6941, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  8300 Cost :  tensor(3.6768, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  8400 Cost :  tensor(3.6597, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  8500 Cost :  tensor(3.6427, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  8600 Cost :  tensor(3.6257, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  8700 Cost :  tensor(3.6088, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  8800 Cost :  tensor(3.5921, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  8900 Cost :  tensor(3.5755, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  9000 Cost :  tensor(3.5589, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  9100 Cost :  tensor(3.5424, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  9200 Cost :  tensor(3.5260, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  9300 Cost :  tensor(3.5097, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  9400 Cost :  tensor(3.4935, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  9500 Cost :  tensor(3.4773, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  9600 Cost :  tensor(3.4613, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  9700 Cost :  tensor(3.4453, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  9800 Cost :  tensor(3.4294, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  9900 Cost :  tensor(3.4136, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  10000 Cost :  tensor(3.3980, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  10100 Cost :  tensor(3.3823, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  10200 Cost :  tensor(3.3668, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  10300 Cost :  tensor(3.3514, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  10400 Cost :  tensor(3.3360, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  10500 Cost :  tensor(3.3208, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  10600 Cost :  tensor(3.3056, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  10700 Cost :  tensor(3.2904, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  10800 Cost :  tensor(3.2754, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  10900 Cost :  tensor(3.2605, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  11000 Cost :  tensor(3.2456, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  11100 Cost :  tensor(3.2308, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  11200 Cost :  tensor(3.2161, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  11300 Cost :  tensor(3.2015, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  11400 Cost :  tensor(3.1869, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  11500 Cost :  tensor(3.1725, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  11600 Cost :  tensor(3.1581, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  11700 Cost :  tensor(3.1437, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  11800 Cost :  tensor(3.1295, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  11900 Cost :  tensor(3.1153, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  12000 Cost :  tensor(3.1012, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  12100 Cost :  tensor(3.0872, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  12200 Cost :  tensor(3.0733, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  12300 Cost :  tensor(3.0594, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  12400 Cost :  tensor(3.0456, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  12500 Cost :  tensor(3.0319, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  12600 Cost :  tensor(3.0183, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  12700 Cost :  tensor(3.0047, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  12800 Cost :  tensor(2.9912, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  12900 Cost :  tensor(2.9777, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  13000 Cost :  tensor(2.9644, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  13100 Cost :  tensor(2.9511, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  13200 Cost :  tensor(2.9379, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  13300 Cost :  tensor(2.9248, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  13400 Cost :  tensor(2.9117, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  13500 Cost :  tensor(2.8987, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  13600 Cost :  tensor(2.8857, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  13700 Cost :  tensor(2.8729, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  13800 Cost :  tensor(2.8601, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  13900 Cost :  tensor(2.8474, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  14000 Cost :  tensor(2.8347, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  14100 Cost :  tensor(2.8221, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  14200 Cost :  tensor(2.8096, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  14300 Cost :  tensor(2.7971, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  14400 Cost :  tensor(2.7847, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  14500 Cost :  tensor(2.7724, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  14600 Cost :  tensor(2.7601, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  14700 Cost :  tensor(2.7479, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  14800 Cost :  tensor(2.7358, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  14900 Cost :  tensor(2.7237, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  15000 Cost :  tensor(2.7116, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  15100 Cost :  tensor(2.6997, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  15200 Cost :  tensor(2.6878, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  15300 Cost :  tensor(2.6760, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  15400 Cost :  tensor(2.6642, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  15500 Cost :  tensor(2.6525, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  15600 Cost :  tensor(2.6409, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  15700 Cost :  tensor(2.6293, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  15800 Cost :  tensor(2.6178, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  15900 Cost :  tensor(2.6063, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  16000 Cost :  tensor(2.5949, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  16100 Cost :  tensor(2.5836, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  16200 Cost :  tensor(2.5723, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  16300 Cost :  tensor(2.5610, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  16400 Cost :  tensor(2.5499, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  16500 Cost :  tensor(2.5388, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  16600 Cost :  tensor(2.5277, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  16700 Cost :  tensor(2.5167, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  16800 Cost :  tensor(2.5058, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  16900 Cost :  tensor(2.4949, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  17000 Cost :  tensor(2.4841, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  17100 Cost :  tensor(2.4733, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  17200 Cost :  tensor(2.4626, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  17300 Cost :  tensor(2.4519, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  17400 Cost :  tensor(2.4414, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  17500 Cost :  tensor(2.4308, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  17600 Cost :  tensor(2.4203, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  17700 Cost :  tensor(2.4099, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  17800 Cost :  tensor(2.3995, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  17900 Cost :  tensor(2.3892, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  18000 Cost :  tensor(2.3789, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  18100 Cost :  tensor(2.3686, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  18200 Cost :  tensor(2.3585, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  18300 Cost :  tensor(2.3484, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  18400 Cost :  tensor(2.3383, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  18500 Cost :  tensor(2.3283, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  18600 Cost :  tensor(2.3183, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  18700 Cost :  tensor(2.3084, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  18800 Cost :  tensor(2.2985, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  18900 Cost :  tensor(2.2887, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  19000 Cost :  tensor(2.2789, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  19100 Cost :  tensor(2.2692, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  19200 Cost :  tensor(2.2596, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  19300 Cost :  tensor(2.2499, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  19400 Cost :  tensor(2.2404, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  19500 Cost :  tensor(2.2309, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  19600 Cost :  tensor(2.2214, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  19700 Cost :  tensor(2.2119, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  19800 Cost :  tensor(2.2026, grad_fn=<MseLossBackward0>)\n",
      "Epoch :  19900 Cost :  tensor(2.1933, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nb_epochs=20000\n",
    "for epoch in range(nb_epochs):\n",
    "    prediction=model(x_train)\n",
    "    cost=F.mse_loss(prediction, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0 :\n",
    "        print('Epoch : ', epoch, 'Cost : ' ,cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[169.3778]], grad_fn=<AddmmBackward0>)\n",
      "[Parameter containing:\n",
      "tensor([[0.8652, 0.8891, 0.2638]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3560], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "new_data=torch.FloatTensor([[80,90,75]])\n",
    "pred_y=model(new_data)\n",
    "print(pred_y)\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  80.,  75.],\n",
      "        [ 93.,  88.,  93.],\n",
      "        [ 89.,  91.,  90.],\n",
      "        [ 96.,  98., 100.],\n",
      "        [ 73.,  66.,  70.]])\n",
      "tensor([[152.],\n",
      "        [185.],\n",
      "        [180.],\n",
      "        [196.],\n",
      "        [142.]])\n"
     ]
    }
   ],
   "source": [
    "data=np.loadtxt('data/data-01-test-score.csv', delimiter=',',dtype='float32')\n",
    "x_data=data[:,0:-1]\n",
    "t_data=data[:,[-1]]\n",
    "x_train=torch.from_numpy(x_data)\n",
    "t_train=torch.from_numpy(t_data)\n",
    "print(x_train[:5])\n",
    "print(t_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=nn.Linear(3,1)\n",
    "optimizer=optim.SGD(model.parameters(), lr=1e-6)\n",
    "nb_epochs=20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(nb_epochs):\n",
    "    prediction=model(x_train)\n",
    "    cost=F.mse_loss(prediction, t_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 400 == 0:\n",
    "        print('Epoch : ', epoch, 'Cost : ',cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLinearRegressModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(3,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MultiLinearRegressModel()\n",
    "optimizer=optim.SGD(model.parameters(), lr=1e-6)\n",
    "nb_epochs=200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0 Cost :  6.103761196136475\n",
      "Epoch :  400 Cost :  6.103679180145264\n",
      "Epoch :  800 Cost :  6.103564262390137\n",
      "Epoch :  1200 Cost :  6.103457450866699\n",
      "Epoch :  1600 Cost :  6.103351593017578\n",
      "Epoch :  2000 Cost :  6.103254795074463\n",
      "Epoch :  2400 Cost :  6.103149890899658\n",
      "Epoch :  2800 Cost :  6.103065013885498\n",
      "Epoch :  3200 Cost :  6.102963924407959\n",
      "Epoch :  3600 Cost :  6.102874755859375\n",
      "Epoch :  4000 Cost :  6.102777481079102\n",
      "Epoch :  4400 Cost :  6.102680683135986\n",
      "Epoch :  4800 Cost :  6.102586269378662\n",
      "Epoch :  5200 Cost :  6.102502346038818\n",
      "Epoch :  5600 Cost :  6.102409839630127\n",
      "Epoch :  6000 Cost :  6.102312088012695\n",
      "Epoch :  6400 Cost :  6.1022233963012695\n",
      "Epoch :  6800 Cost :  6.102149486541748\n",
      "Epoch :  7200 Cost :  6.102057933807373\n",
      "Epoch :  7600 Cost :  6.101968765258789\n",
      "Epoch :  8000 Cost :  6.10188102722168\n",
      "Epoch :  8400 Cost :  6.101789474487305\n",
      "Epoch :  8800 Cost :  6.101713180541992\n",
      "Epoch :  9200 Cost :  6.1016435623168945\n",
      "Epoch :  9600 Cost :  6.101556777954102\n",
      "Epoch :  10000 Cost :  6.101490020751953\n",
      "Epoch :  10400 Cost :  6.101407051086426\n",
      "Epoch :  10800 Cost :  6.101346015930176\n",
      "Epoch :  11200 Cost :  6.101266860961914\n",
      "Epoch :  11600 Cost :  6.101198673248291\n",
      "Epoch :  12000 Cost :  6.101130485534668\n",
      "Epoch :  12400 Cost :  6.101069927215576\n",
      "Epoch :  12800 Cost :  6.101010322570801\n",
      "Epoch :  13200 Cost :  6.100946426391602\n",
      "Epoch :  13600 Cost :  6.100886344909668\n",
      "Epoch :  14000 Cost :  6.100822925567627\n",
      "Epoch :  14400 Cost :  6.100757598876953\n",
      "Epoch :  14800 Cost :  6.100706100463867\n",
      "Epoch :  15200 Cost :  6.1006646156311035\n",
      "Epoch :  15600 Cost :  6.1005964279174805\n",
      "Epoch :  16000 Cost :  6.100533485412598\n",
      "Epoch :  16400 Cost :  6.100490093231201\n",
      "Epoch :  16800 Cost :  6.100433826446533\n",
      "Epoch :  17200 Cost :  6.10037088394165\n",
      "Epoch :  17600 Cost :  6.100334644317627\n",
      "Epoch :  18000 Cost :  6.100292205810547\n",
      "Epoch :  18400 Cost :  6.100243091583252\n",
      "Epoch :  18800 Cost :  6.100179672241211\n",
      "Epoch :  19200 Cost :  6.100149154663086\n",
      "Epoch :  19600 Cost :  6.100088596343994\n",
      "Epoch :  20000 Cost :  6.100059986114502\n",
      "Epoch :  20400 Cost :  6.099992275238037\n",
      "Epoch :  20800 Cost :  6.09995174407959\n",
      "Epoch :  21200 Cost :  6.099907875061035\n",
      "Epoch :  21600 Cost :  6.0998687744140625\n",
      "Epoch :  22000 Cost :  6.099823474884033\n",
      "Epoch :  22400 Cost :  6.099787712097168\n",
      "Epoch :  22800 Cost :  6.099734306335449\n",
      "Epoch :  23200 Cost :  6.099693775177002\n",
      "Epoch :  23600 Cost :  6.099652290344238\n",
      "Epoch :  24000 Cost :  6.099615097045898\n",
      "Epoch :  24400 Cost :  6.0995774269104\n",
      "Epoch :  24800 Cost :  6.0995330810546875\n",
      "Epoch :  25200 Cost :  6.099495887756348\n",
      "Epoch :  25600 Cost :  6.09946346282959\n",
      "Epoch :  26000 Cost :  6.099416732788086\n",
      "Epoch :  26400 Cost :  6.099373817443848\n",
      "Epoch :  26800 Cost :  6.099336624145508\n",
      "Epoch :  27200 Cost :  6.099308013916016\n",
      "Epoch :  27600 Cost :  6.099257946014404\n",
      "Epoch :  28000 Cost :  6.099239349365234\n",
      "Epoch :  28400 Cost :  6.099194526672363\n",
      "Epoch :  28800 Cost :  6.099180698394775\n",
      "Epoch :  29200 Cost :  6.099127769470215\n",
      "Epoch :  29600 Cost :  6.099109649658203\n",
      "Epoch :  30000 Cost :  6.099052906036377\n",
      "Epoch :  30400 Cost :  6.0990424156188965\n",
      "Epoch :  30800 Cost :  6.0989990234375\n",
      "Epoch :  31200 Cost :  6.098974704742432\n",
      "Epoch :  31600 Cost :  6.098935127258301\n",
      "Epoch :  32000 Cost :  6.0989155769348145\n",
      "Epoch :  32400 Cost :  6.098876953125\n",
      "Epoch :  32800 Cost :  6.098841667175293\n",
      "Epoch :  33200 Cost :  6.0988264083862305\n",
      "Epoch :  33600 Cost :  6.098787784576416\n",
      "Epoch :  34000 Cost :  6.098760604858398\n",
      "Epoch :  34400 Cost :  6.098728179931641\n",
      "Epoch :  34800 Cost :  6.098687648773193\n",
      "Epoch :  35200 Cost :  6.0986762046813965\n",
      "Epoch :  35600 Cost :  6.098629951477051\n",
      "Epoch :  36000 Cost :  6.098611354827881\n",
      "Epoch :  36400 Cost :  6.098569869995117\n",
      "Epoch :  36800 Cost :  6.098552703857422\n",
      "Epoch :  37200 Cost :  6.098520278930664\n",
      "Epoch :  37600 Cost :  6.098491668701172\n",
      "Epoch :  38000 Cost :  6.098458290100098\n",
      "Epoch :  38400 Cost :  6.098435878753662\n",
      "Epoch :  38800 Cost :  6.098414897918701\n",
      "Epoch :  39200 Cost :  6.0983781814575195\n",
      "Epoch :  39600 Cost :  6.098353862762451\n",
      "Epoch :  40000 Cost :  6.098331451416016\n",
      "Epoch :  40400 Cost :  6.098288536071777\n",
      "Epoch :  40800 Cost :  6.098268985748291\n",
      "Epoch :  41200 Cost :  6.098247528076172\n",
      "Epoch :  41600 Cost :  6.098219394683838\n",
      "Epoch :  42000 Cost :  6.098207950592041\n",
      "Epoch :  42400 Cost :  6.09817361831665\n",
      "Epoch :  42800 Cost :  6.098148345947266\n",
      "Epoch :  43200 Cost :  6.098130702972412\n",
      "Epoch :  43600 Cost :  6.0981011390686035\n",
      "Epoch :  44000 Cost :  6.098076343536377\n",
      "Epoch :  44400 Cost :  6.098061561584473\n",
      "Epoch :  44800 Cost :  6.0980329513549805\n",
      "Epoch :  45200 Cost :  6.098012924194336\n",
      "Epoch :  45600 Cost :  6.097989082336426\n",
      "Epoch :  46000 Cost :  6.097972393035889\n",
      "Epoch :  46400 Cost :  6.097960948944092\n",
      "Epoch :  46800 Cost :  6.097926616668701\n",
      "Epoch :  47200 Cost :  6.097912788391113\n",
      "Epoch :  47600 Cost :  6.097893714904785\n",
      "Epoch :  48000 Cost :  6.097877025604248\n",
      "Epoch :  48400 Cost :  6.0978474617004395\n",
      "Epoch :  48800 Cost :  6.0978264808654785\n",
      "Epoch :  49200 Cost :  6.097804546356201\n",
      "Epoch :  49600 Cost :  6.097795009613037\n",
      "Epoch :  50000 Cost :  6.097789764404297\n",
      "Epoch :  50400 Cost :  6.097769260406494\n",
      "Epoch :  50800 Cost :  6.0977349281311035\n",
      "Epoch :  51200 Cost :  6.097720146179199\n",
      "Epoch :  51600 Cost :  6.097700119018555\n",
      "Epoch :  52000 Cost :  6.0976881980896\n",
      "Epoch :  52400 Cost :  6.097675323486328\n",
      "Epoch :  52800 Cost :  6.097639560699463\n",
      "Epoch :  53200 Cost :  6.0976243019104\n",
      "Epoch :  53600 Cost :  6.0976128578186035\n",
      "Epoch :  54000 Cost :  6.09759521484375\n",
      "Epoch :  54400 Cost :  6.097574234008789\n",
      "Epoch :  54800 Cost :  6.097560405731201\n",
      "Epoch :  55200 Cost :  6.0975341796875\n",
      "Epoch :  55600 Cost :  6.097531795501709\n",
      "Epoch :  56000 Cost :  6.097519397735596\n",
      "Epoch :  56400 Cost :  6.097499847412109\n",
      "Epoch :  56800 Cost :  6.09747314453125\n",
      "Epoch :  57200 Cost :  6.097456455230713\n",
      "Epoch :  57600 Cost :  6.097446441650391\n",
      "Epoch :  58000 Cost :  6.097431182861328\n",
      "Epoch :  58400 Cost :  6.097406387329102\n",
      "Epoch :  58800 Cost :  6.097400665283203\n",
      "Epoch :  59200 Cost :  6.097377300262451\n",
      "Epoch :  59600 Cost :  6.097370147705078\n",
      "Epoch :  60000 Cost :  6.097356796264648\n",
      "Epoch :  60400 Cost :  6.09732723236084\n",
      "Epoch :  60800 Cost :  6.097317695617676\n",
      "Epoch :  61200 Cost :  6.0973052978515625\n",
      "Epoch :  61600 Cost :  6.097292900085449\n",
      "Epoch :  62000 Cost :  6.0972795486450195\n",
      "Epoch :  62400 Cost :  6.097266674041748\n",
      "Epoch :  62800 Cost :  6.097242832183838\n",
      "Epoch :  63200 Cost :  6.097230434417725\n",
      "Epoch :  63600 Cost :  6.097204208374023\n",
      "Epoch :  64000 Cost :  6.097198486328125\n",
      "Epoch :  64400 Cost :  6.097179412841797\n",
      "Epoch :  64800 Cost :  6.09718132019043\n",
      "Epoch :  65200 Cost :  6.097161769866943\n",
      "Epoch :  65600 Cost :  6.097152233123779\n",
      "Epoch :  66000 Cost :  6.0971245765686035\n",
      "Epoch :  66400 Cost :  6.097111701965332\n",
      "Epoch :  66800 Cost :  6.0970869064331055\n",
      "Epoch :  67200 Cost :  6.097090244293213\n",
      "Epoch :  67600 Cost :  6.0970778465271\n",
      "Epoch :  68000 Cost :  6.097048759460449\n",
      "Epoch :  68400 Cost :  6.097046375274658\n",
      "Epoch :  68800 Cost :  6.097022533416748\n",
      "Epoch :  69200 Cost :  6.097022533416748\n",
      "Epoch :  69600 Cost :  6.097003936767578\n",
      "Epoch :  70000 Cost :  6.096982955932617\n",
      "Epoch :  70400 Cost :  6.096977710723877\n",
      "Epoch :  70800 Cost :  6.096969127655029\n",
      "Epoch :  71200 Cost :  6.096949577331543\n",
      "Epoch :  71600 Cost :  6.096930503845215\n",
      "Epoch :  72000 Cost :  6.096926212310791\n",
      "Epoch :  72400 Cost :  6.096909999847412\n",
      "Epoch :  72800 Cost :  6.096889495849609\n",
      "Epoch :  73200 Cost :  6.096871852874756\n",
      "Epoch :  73600 Cost :  6.096884727478027\n",
      "Epoch :  74000 Cost :  6.096854209899902\n",
      "Epoch :  74400 Cost :  6.0968499183654785\n",
      "Epoch :  74800 Cost :  6.096826553344727\n",
      "Epoch :  75200 Cost :  6.096823692321777\n",
      "Epoch :  75600 Cost :  6.096807956695557\n",
      "Epoch :  76000 Cost :  6.096778392791748\n",
      "Epoch :  76400 Cost :  6.096778392791748\n",
      "Epoch :  76800 Cost :  6.096761703491211\n",
      "Epoch :  77200 Cost :  6.096750259399414\n",
      "Epoch :  77600 Cost :  6.096738338470459\n",
      "Epoch :  78000 Cost :  6.096714496612549\n",
      "Epoch :  78400 Cost :  6.09671688079834\n",
      "Epoch :  78800 Cost :  6.096685409545898\n",
      "Epoch :  79200 Cost :  6.096673488616943\n",
      "Epoch :  79600 Cost :  6.0966715812683105\n",
      "Epoch :  80000 Cost :  6.096657752990723\n",
      "Epoch :  80400 Cost :  6.096651554107666\n",
      "Epoch :  80800 Cost :  6.096634387969971\n",
      "Epoch :  81200 Cost :  6.0966315269470215\n",
      "Epoch :  81600 Cost :  6.09661865234375\n",
      "Epoch :  82000 Cost :  6.096592903137207\n",
      "Epoch :  82400 Cost :  6.096574783325195\n",
      "Epoch :  82800 Cost :  6.096583843231201\n",
      "Epoch :  83200 Cost :  6.096563339233398\n",
      "Epoch :  83600 Cost :  6.0965495109558105\n",
      "Epoch :  84000 Cost :  6.096536636352539\n",
      "Epoch :  84400 Cost :  6.096529483795166\n",
      "Epoch :  84800 Cost :  6.096515655517578\n",
      "Epoch :  85200 Cost :  6.096505165100098\n",
      "Epoch :  85600 Cost :  6.096494197845459\n",
      "Epoch :  86000 Cost :  6.096472263336182\n",
      "Epoch :  86400 Cost :  6.096470832824707\n",
      "Epoch :  86800 Cost :  6.096450328826904\n",
      "Epoch :  87200 Cost :  6.096440315246582\n",
      "Epoch :  87600 Cost :  6.096431732177734\n",
      "Epoch :  88000 Cost :  6.096416473388672\n",
      "Epoch :  88400 Cost :  6.096417427062988\n",
      "Epoch :  88800 Cost :  6.096395969390869\n",
      "Epoch :  89200 Cost :  6.096369743347168\n",
      "Epoch :  89600 Cost :  6.096367835998535\n",
      "Epoch :  90000 Cost :  6.096358776092529\n",
      "Epoch :  90400 Cost :  6.0963454246521\n",
      "Epoch :  90800 Cost :  6.096341133117676\n",
      "Epoch :  91200 Cost :  6.096324920654297\n",
      "Epoch :  91600 Cost :  6.096311569213867\n",
      "Epoch :  92000 Cost :  6.09630012512207\n",
      "Epoch :  92400 Cost :  6.096278190612793\n",
      "Epoch :  92800 Cost :  6.096266269683838\n",
      "Epoch :  93200 Cost :  6.096256256103516\n",
      "Epoch :  93600 Cost :  6.096248626708984\n",
      "Epoch :  94000 Cost :  6.096241474151611\n",
      "Epoch :  94400 Cost :  6.096225738525391\n",
      "Epoch :  94800 Cost :  6.096221446990967\n",
      "Epoch :  95200 Cost :  6.096202373504639\n",
      "Epoch :  95600 Cost :  6.096197605133057\n",
      "Epoch :  96000 Cost :  6.096175670623779\n",
      "Epoch :  96400 Cost :  6.0961761474609375\n",
      "Epoch :  96800 Cost :  6.0961527824401855\n",
      "Epoch :  97200 Cost :  6.096137523651123\n",
      "Epoch :  97600 Cost :  6.096128463745117\n",
      "Epoch :  98000 Cost :  6.096128940582275\n",
      "Epoch :  98400 Cost :  6.096112251281738\n",
      "Epoch :  98800 Cost :  6.096096992492676\n",
      "Epoch :  99200 Cost :  6.096090316772461\n",
      "Epoch :  99600 Cost :  6.0960774421691895\n",
      "Epoch :  100000 Cost :  6.0960540771484375\n",
      "Epoch :  100400 Cost :  6.0960469245910645\n",
      "Epoch :  100800 Cost :  6.09604549407959\n",
      "Epoch :  101200 Cost :  6.096013069152832\n",
      "Epoch :  101600 Cost :  6.096009731292725\n",
      "Epoch :  102000 Cost :  6.096001625061035\n",
      "Epoch :  102400 Cost :  6.095982074737549\n",
      "Epoch :  102800 Cost :  6.095977783203125\n",
      "Epoch :  103200 Cost :  6.09597110748291\n",
      "Epoch :  103600 Cost :  6.095951080322266\n",
      "Epoch :  104000 Cost :  6.095944881439209\n",
      "Epoch :  104400 Cost :  6.095938205718994\n",
      "Epoch :  104800 Cost :  6.095921039581299\n",
      "Epoch :  105200 Cost :  6.095908164978027\n",
      "Epoch :  105600 Cost :  6.095905780792236\n",
      "Epoch :  106000 Cost :  6.095886707305908\n",
      "Epoch :  106400 Cost :  6.095871448516846\n",
      "Epoch :  106800 Cost :  6.095865249633789\n",
      "Epoch :  107200 Cost :  6.09584903717041\n",
      "Epoch :  107600 Cost :  6.095839023590088\n",
      "Epoch :  108000 Cost :  6.095827579498291\n",
      "Epoch :  108400 Cost :  6.095820426940918\n",
      "Epoch :  108800 Cost :  6.095809459686279\n",
      "Epoch :  109200 Cost :  6.095788955688477\n",
      "Epoch :  109600 Cost :  6.095783710479736\n",
      "Epoch :  110000 Cost :  6.095771312713623\n",
      "Epoch :  110400 Cost :  6.095755577087402\n",
      "Epoch :  110800 Cost :  6.095750331878662\n",
      "Epoch :  111200 Cost :  6.095746040344238\n",
      "Epoch :  111600 Cost :  6.095733642578125\n",
      "Epoch :  112000 Cost :  6.095728874206543\n",
      "Epoch :  112400 Cost :  6.095709800720215\n",
      "Epoch :  112800 Cost :  6.095698833465576\n",
      "Epoch :  113200 Cost :  6.095682144165039\n",
      "Epoch :  113600 Cost :  6.095675468444824\n",
      "Epoch :  114000 Cost :  6.095661163330078\n",
      "Epoch :  114400 Cost :  6.095650672912598\n",
      "Epoch :  114800 Cost :  6.095644474029541\n",
      "Epoch :  115200 Cost :  6.095631122589111\n",
      "Epoch :  115600 Cost :  6.0956220626831055\n",
      "Epoch :  116000 Cost :  6.095616340637207\n",
      "Epoch :  116400 Cost :  6.095605373382568\n",
      "Epoch :  116800 Cost :  6.095583915710449\n",
      "Epoch :  117200 Cost :  6.095576763153076\n",
      "Epoch :  117600 Cost :  6.095554351806641\n",
      "Epoch :  118000 Cost :  6.095554351806641\n",
      "Epoch :  118400 Cost :  6.095541477203369\n",
      "Epoch :  118800 Cost :  6.0955305099487305\n",
      "Epoch :  119200 Cost :  6.095521450042725\n",
      "Epoch :  119600 Cost :  6.095509052276611\n",
      "Epoch :  120000 Cost :  6.0955047607421875\n",
      "Epoch :  120400 Cost :  6.09548807144165\n",
      "Epoch :  120800 Cost :  6.095478057861328\n",
      "Epoch :  121200 Cost :  6.095460891723633\n",
      "Epoch :  121600 Cost :  6.095449924468994\n",
      "Epoch :  122000 Cost :  6.095442295074463\n",
      "Epoch :  122400 Cost :  6.095432758331299\n",
      "Epoch :  122800 Cost :  6.095423698425293\n",
      "Epoch :  123200 Cost :  6.0954203605651855\n",
      "Epoch :  123600 Cost :  6.0954108238220215\n",
      "Epoch :  124000 Cost :  6.09539794921875\n",
      "Epoch :  124400 Cost :  6.095378398895264\n",
      "Epoch :  124800 Cost :  6.095369338989258\n",
      "Epoch :  125200 Cost :  6.095351696014404\n",
      "Epoch :  125600 Cost :  6.095340728759766\n",
      "Epoch :  126000 Cost :  6.095328330993652\n",
      "Epoch :  126400 Cost :  6.095308303833008\n",
      "Epoch :  126800 Cost :  6.095303535461426\n",
      "Epoch :  127200 Cost :  6.0952982902526855\n",
      "Epoch :  127600 Cost :  6.095294952392578\n",
      "Epoch :  128000 Cost :  6.095278263092041\n",
      "Epoch :  128400 Cost :  6.095268726348877\n",
      "Epoch :  128800 Cost :  6.095258712768555\n",
      "Epoch :  129200 Cost :  6.095250129699707\n",
      "Epoch :  129600 Cost :  6.095236301422119\n",
      "Epoch :  130000 Cost :  6.095223903656006\n",
      "Epoch :  130400 Cost :  6.095214366912842\n",
      "Epoch :  130800 Cost :  6.095195770263672\n",
      "Epoch :  131200 Cost :  6.095202445983887\n",
      "Epoch :  131600 Cost :  6.095167636871338\n",
      "Epoch :  132000 Cost :  6.095174789428711\n",
      "Epoch :  132400 Cost :  6.095151424407959\n",
      "Epoch :  132800 Cost :  6.095153331756592\n",
      "Epoch :  133200 Cost :  6.09514045715332\n",
      "Epoch :  133600 Cost :  6.095125198364258\n",
      "Epoch :  134000 Cost :  6.09512186050415\n",
      "Epoch :  134400 Cost :  6.095098972320557\n",
      "Epoch :  134800 Cost :  6.095099925994873\n",
      "Epoch :  135200 Cost :  6.095088481903076\n",
      "Epoch :  135600 Cost :  6.095071315765381\n",
      "Epoch :  136000 Cost :  6.095056056976318\n",
      "Epoch :  136400 Cost :  6.095058441162109\n",
      "Epoch :  136800 Cost :  6.095036029815674\n",
      "Epoch :  137200 Cost :  6.095029830932617\n",
      "Epoch :  137600 Cost :  6.095014572143555\n",
      "Epoch :  138000 Cost :  6.095012187957764\n",
      "Epoch :  138400 Cost :  6.094990253448486\n",
      "Epoch :  138800 Cost :  6.094986438751221\n",
      "Epoch :  139200 Cost :  6.094972610473633\n",
      "Epoch :  139600 Cost :  6.094961643218994\n",
      "Epoch :  140000 Cost :  6.094953536987305\n",
      "Epoch :  140400 Cost :  6.094944953918457\n",
      "Epoch :  140800 Cost :  6.0949249267578125\n",
      "Epoch :  141200 Cost :  6.094916343688965\n",
      "Epoch :  141600 Cost :  6.094905376434326\n",
      "Epoch :  142000 Cost :  6.094910144805908\n",
      "Epoch :  142400 Cost :  6.094887733459473\n",
      "Epoch :  142800 Cost :  6.094874858856201\n",
      "Epoch :  143200 Cost :  6.094870567321777\n",
      "Epoch :  143600 Cost :  6.094859600067139\n",
      "Epoch :  144000 Cost :  6.094840049743652\n",
      "Epoch :  144400 Cost :  6.094834804534912\n",
      "Epoch :  144800 Cost :  6.094825267791748\n",
      "Epoch :  145200 Cost :  6.094809055328369\n",
      "Epoch :  145600 Cost :  6.094789505004883\n",
      "Epoch :  146000 Cost :  6.094786167144775\n",
      "Epoch :  146400 Cost :  6.094779014587402\n",
      "Epoch :  146800 Cost :  6.094757080078125\n",
      "Epoch :  147200 Cost :  6.09476375579834\n",
      "Epoch :  147600 Cost :  6.094750881195068\n",
      "Epoch :  148000 Cost :  6.094731330871582\n",
      "Epoch :  148400 Cost :  6.094732284545898\n",
      "Epoch :  148800 Cost :  6.094712734222412\n",
      "Epoch :  149200 Cost :  6.0947041511535645\n",
      "Epoch :  149600 Cost :  6.094693660736084\n",
      "Epoch :  150000 Cost :  6.094676494598389\n",
      "Epoch :  150400 Cost :  6.094666004180908\n",
      "Epoch :  150800 Cost :  6.094661235809326\n",
      "Epoch :  151200 Cost :  6.094651699066162\n",
      "Epoch :  151600 Cost :  6.094644546508789\n",
      "Epoch :  152000 Cost :  6.094616889953613\n",
      "Epoch :  152400 Cost :  6.094622611999512\n",
      "Epoch :  152800 Cost :  6.094598293304443\n",
      "Epoch :  153200 Cost :  6.094586372375488\n",
      "Epoch :  153600 Cost :  6.0945820808410645\n",
      "Epoch :  154000 Cost :  6.094560146331787\n",
      "Epoch :  154400 Cost :  6.094560146331787\n",
      "Epoch :  154800 Cost :  6.094539642333984\n",
      "Epoch :  155200 Cost :  6.094541072845459\n",
      "Epoch :  155600 Cost :  6.094525337219238\n",
      "Epoch :  156000 Cost :  6.094516754150391\n",
      "Epoch :  156400 Cost :  6.094501495361328\n",
      "Epoch :  156800 Cost :  6.094498157501221\n",
      "Epoch :  157200 Cost :  6.094485282897949\n",
      "Epoch :  157600 Cost :  6.094472885131836\n",
      "Epoch :  158000 Cost :  6.094455718994141\n",
      "Epoch :  158400 Cost :  6.094447135925293\n",
      "Epoch :  158800 Cost :  6.094438076019287\n",
      "Epoch :  159200 Cost :  6.0944342613220215\n",
      "Epoch :  159600 Cost :  6.094410419464111\n",
      "Epoch :  160000 Cost :  6.094402313232422\n",
      "Epoch :  160400 Cost :  6.094395637512207\n",
      "Epoch :  160800 Cost :  6.094391345977783\n",
      "Epoch :  161200 Cost :  6.094379901885986\n",
      "Epoch :  161600 Cost :  6.094356536865234\n",
      "Epoch :  162000 Cost :  6.094350814819336\n",
      "Epoch :  162400 Cost :  6.094338893890381\n",
      "Epoch :  162800 Cost :  6.094339370727539\n",
      "Epoch :  163200 Cost :  6.094320297241211\n",
      "Epoch :  163600 Cost :  6.094296455383301\n",
      "Epoch :  164000 Cost :  6.09428596496582\n",
      "Epoch :  164400 Cost :  6.0942792892456055\n",
      "Epoch :  164800 Cost :  6.094285488128662\n",
      "Epoch :  165200 Cost :  6.094264030456543\n",
      "Epoch :  165600 Cost :  6.094253063201904\n",
      "Epoch :  166000 Cost :  6.0942487716674805\n",
      "Epoch :  166400 Cost :  6.094234466552734\n",
      "Epoch :  166800 Cost :  6.094229698181152\n",
      "Epoch :  167200 Cost :  6.094217300415039\n",
      "Epoch :  167600 Cost :  6.0941996574401855\n",
      "Epoch :  168000 Cost :  6.094181060791016\n",
      "Epoch :  168400 Cost :  6.094177722930908\n",
      "Epoch :  168800 Cost :  6.09416389465332\n",
      "Epoch :  169200 Cost :  6.094155311584473\n",
      "Epoch :  169600 Cost :  6.094137191772461\n",
      "Epoch :  170000 Cost :  6.094133377075195\n",
      "Epoch :  170400 Cost :  6.094128608703613\n",
      "Epoch :  170800 Cost :  6.094107151031494\n",
      "Epoch :  171200 Cost :  6.094099521636963\n",
      "Epoch :  171600 Cost :  6.094089984893799\n",
      "Epoch :  172000 Cost :  6.094081401824951\n",
      "Epoch :  172400 Cost :  6.094077110290527\n",
      "Epoch :  172800 Cost :  6.094062328338623\n",
      "Epoch :  173200 Cost :  6.094046592712402\n",
      "Epoch :  173600 Cost :  6.094027042388916\n",
      "Epoch :  174000 Cost :  6.094020366668701\n",
      "Epoch :  174400 Cost :  6.094033718109131\n",
      "Epoch :  174800 Cost :  6.094005584716797\n",
      "Epoch :  175200 Cost :  6.093990325927734\n",
      "Epoch :  175600 Cost :  6.093994617462158\n",
      "Epoch :  176000 Cost :  6.093976974487305\n",
      "Epoch :  176400 Cost :  6.0939621925354\n",
      "Epoch :  176800 Cost :  6.0939412117004395\n",
      "Epoch :  177200 Cost :  6.093939304351807\n",
      "Epoch :  177600 Cost :  6.093920707702637\n",
      "Epoch :  178000 Cost :  6.093911170959473\n",
      "Epoch :  178400 Cost :  6.09390926361084\n",
      "Epoch :  178800 Cost :  6.093890190124512\n",
      "Epoch :  179200 Cost :  6.093879222869873\n",
      "Epoch :  179600 Cost :  6.093880653381348\n",
      "Epoch :  180000 Cost :  6.093863487243652\n",
      "Epoch :  180400 Cost :  6.093851089477539\n",
      "Epoch :  180800 Cost :  6.0938520431518555\n",
      "Epoch :  181200 Cost :  6.093830108642578\n",
      "Epoch :  181600 Cost :  6.093822002410889\n",
      "Epoch :  182000 Cost :  6.0938029289245605\n",
      "Epoch :  182400 Cost :  6.093800067901611\n",
      "Epoch :  182800 Cost :  6.093782424926758\n",
      "Epoch :  183200 Cost :  6.093770980834961\n",
      "Epoch :  183600 Cost :  6.093770980834961\n",
      "Epoch :  184000 Cost :  6.093753814697266\n",
      "Epoch :  184400 Cost :  6.09373664855957\n",
      "Epoch :  184800 Cost :  6.0937347412109375\n",
      "Epoch :  185200 Cost :  6.093726634979248\n",
      "Epoch :  185600 Cost :  6.0937066078186035\n",
      "Epoch :  186000 Cost :  6.093700408935547\n",
      "Epoch :  186400 Cost :  6.093686580657959\n",
      "Epoch :  186800 Cost :  6.0936784744262695\n",
      "Epoch :  187200 Cost :  6.09366512298584\n",
      "Epoch :  187600 Cost :  6.0936665534973145\n",
      "Epoch :  188000 Cost :  6.0936479568481445\n",
      "Epoch :  188400 Cost :  6.093625068664551\n",
      "Epoch :  188800 Cost :  6.093625545501709\n",
      "Epoch :  189200 Cost :  6.09361457824707\n",
      "Epoch :  189600 Cost :  6.09360408782959\n",
      "Epoch :  190000 Cost :  6.0935893058776855\n",
      "Epoch :  190400 Cost :  6.093575477600098\n",
      "Epoch :  190800 Cost :  6.093568325042725\n",
      "Epoch :  191200 Cost :  6.093562602996826\n",
      "Epoch :  191600 Cost :  6.093541145324707\n",
      "Epoch :  192000 Cost :  6.093540191650391\n",
      "Epoch :  192400 Cost :  6.093527793884277\n",
      "Epoch :  192800 Cost :  6.0935139656066895\n",
      "Epoch :  193200 Cost :  6.093514919281006\n",
      "Epoch :  193600 Cost :  6.093501091003418\n",
      "Epoch :  194000 Cost :  6.093490123748779\n",
      "Epoch :  194400 Cost :  6.093473434448242\n",
      "Epoch :  194800 Cost :  6.093461990356445\n",
      "Epoch :  195200 Cost :  6.093453884124756\n",
      "Epoch :  195600 Cost :  6.093438148498535\n",
      "Epoch :  196000 Cost :  6.093435764312744\n",
      "Epoch :  196400 Cost :  6.093416690826416\n",
      "Epoch :  196800 Cost :  6.093414306640625\n",
      "Epoch :  197200 Cost :  6.093403339385986\n",
      "Epoch :  197600 Cost :  6.093393325805664\n",
      "Epoch :  198000 Cost :  6.093374252319336\n",
      "Epoch :  198400 Cost :  6.093364715576172\n",
      "Epoch :  198800 Cost :  6.093358039855957\n",
      "Epoch :  199200 Cost :  6.093338489532471\n",
      "Epoch :  199600 Cost :  6.093331813812256\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nb_epochs):\n",
    "    prediction=model(x_train)\n",
    "    cost=F.mse_loss(prediction, t_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 400==0:\n",
    "        print('Epoch : ', epoch, 'Cost : ', cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[152.9395]], grad_fn=<AddmmBackward0>)\n",
      "[Parameter containing:\n",
      "tensor([[0.3575, 0.5300, 1.1264]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0426], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "new_data=torch.FloatTensor([[73,80,75]])\n",
    "pred_y=model(new_data)\n",
    "print(pred_y)\n",
    "print(list(model.parameters()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
